{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Tokens', 'Intent', 'ner_labels'],\n",
       "        num_rows: 2245\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Tokens', 'Intent', 'ner_labels'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset('json', data_files='data/train_data.json')\n",
    "model_checkpoint = 'VietAI/vit5-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, max_length=256)\n",
    "\n",
    "train_size = int(0.9 * len(dataset['train']))\n",
    "dataset['train'].shuffle()\n",
    "train_dataset = dataset['train'].select(range(train_size))\n",
    "val_dataset = dataset['train'].select(range(train_size, len(dataset['train'])))\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 19, 20, 20, 20, 0, 5, 6, 0, 0, 3, 4, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 19, 20, 20, 20, 0, 5, 6, 0, 0, 3, 4, 0, 0, 0, 0, 1, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "all_labels = ['O', 'B-balcony_direction','I-balcony_direction','B-city','I-city','B-district','I-district','B-house_direction','I-house_direction', 'B-legal','I-legal', 'B-max_acreage','I-max_acreage', 'B-max_price','I-max_price','B-min_acreage','I-min_acreage','B-min_price','I-min_price', 'B-type_of_land','I-type_of_land']\n",
    "\n",
    "def create_ner_tags(examples):\n",
    "    \n",
    "    ner_tags = [[all_labels.index(label) for label in labels] for labels in examples[\"ner_labels\"]]\n",
    "\n",
    "    return {\"ner_tags\": ner_tags}\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "raw_datasets = raw_datasets.map(create_ner_tags, batched=True)\n",
    "\n",
    "labels = raw_datasets[\"train\"][45][\"ner_tags\"]\n",
    "inputs = tokenizer(raw_datasets[\"train\"][45][\"Tokens\"], is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f0d183d5ea434d8d97dea7479a1f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2245\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"Tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,    0,    0,    3,    4,    0,    7,    0,    0,   17,\n",
       "           18,    0,   13,   14, -100],\n",
       "        [  19,   20,    0,    0,    0,    3,    4,    0,    7,    0,    0,    9,\n",
       "           10,   10,    0, -100, -100]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForTokenClassification were not initialized from the model checkpoint at VietAI/vit5-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(all_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[all_labels[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebd622fe5b04d61be5b86f205e85f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/843 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc41ef1e7a544361bd0bd2fc7404810d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lg\\miniconda3\\envs\\tova\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4586713910102844, 'eval_precision': 0.9241573033707865, 'eval_recall': 0.996969696969697, 'eval_f1': 0.9591836734693877, 'eval_accuracy': 0.9877712031558186, 'eval_runtime': 3.6307, 'eval_samples_per_second': 68.858, 'eval_steps_per_second': 8.814, 'epoch': 1.0}\n",
      "{'loss': 1.568, 'grad_norm': 24.983171463012695, 'learning_rate': 8.137603795966786e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eac355f72ec4f99a61dbbf62fc71973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lg\\miniconda3\\envs\\tova\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5133143663406372, 'eval_precision': 0.9189944134078212, 'eval_recall': 0.996969696969697, 'eval_f1': 0.9563953488372093, 'eval_accuracy': 0.9881656804733728, 'eval_runtime': 3.4152, 'eval_samples_per_second': 73.202, 'eval_steps_per_second': 9.37, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b120d472ff46359a4b3da2c94e282d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lg\\miniconda3\\envs\\tova\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4648110568523407, 'eval_precision': 0.9189944134078212, 'eval_recall': 0.996969696969697, 'eval_f1': 0.9563953488372093, 'eval_accuracy': 0.9877712031558186, 'eval_runtime': 3.6696, 'eval_samples_per_second': 68.127, 'eval_steps_per_second': 8.72, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1051.1203, 'train_samples_per_second': 6.407, 'train_steps_per_second': 0.802, 'train_loss': 0.933494241495008, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=843, training_loss=0.933494241495008, metrics={'train_runtime': 1051.1203, 'train_samples_per_second': 6.407, 'train_steps_per_second': 0.802, 'total_flos': 90513448126200.0, 'train_loss': 0.933494241495008, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"ViT5-real-estate-ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'city',\n",
       "  'score': 1.0,\n",
       "  'word': 'Hùng',\n",
       "  'start': 10,\n",
       "  'end': 15}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"ViT5-real-estate-ner\"\n",
    "\n",
    "ner = pipeline(\"ner\", model=model_checkpoint, aggregation_strategy=\"simple\", device=0)\n",
    "ner(\"Tôi tên là Hùng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "data = pd.read_excel('data/router.xlsx')\n",
    "\n",
    "client = InferenceClient(api_key=os.environ[\"HF_TOKEN\"])\n",
    "template = [\n",
    "\t{ \"role\": \"system\", \"content\": \"Bạn là một trợ lý ảo chuyên phân loại ý định dựa trên câu chat của người dùng (intent classification). Nếu người dùng muốn tìm nhà, hãy trả lời \\\"[SEARCH_HOUSE]\\\". Còn lại thì trả lời \\\"[NORMAL_CHAT]\\\"\" },\n",
    "\t{ \"role\": \"user\", \"content\": \"Bạn là ai\" },\n",
    "\t{ \"role\": \"assistant\", \"content\": \"[NORMAL_CHAT]\" },\n",
    "\t{ \"role\": \"user\", \"content\": \"Tìm nhà ở Hà Nội\" },\n",
    "\t{ \"role\": \"assistant\", \"content\": \"[SEARCH_HOUSE]\" },\n",
    "\t{ \"role\": \"user\", \"content\": \"Nhà cấp 4 có dành cho người thu nhập thấp?\" },\n",
    "\t{ \"role\": \"assistant\", \"content\": \"[NORMAL_CHAT]\" },\n",
    "\t{ \"role\": \"user\", \"content\": \"Cho tôi thông tin về chung cư mini ở Hồ Chí Minh\" },\n",
    "\t{ \"role\": \"assistant\", \"content\": \"[SEARCH_HOUSE]\" },\n",
    "\t{ \"role\": \"user\", \"content\": \"Bạn nghĩ làm thế nào để tiết kiệm chi phí mua nhà?\" },\n",
    "\t{ \"role\": \"assistant\", \"content\": \"[NORMAL_CHAT]\" },\n",
    "\t{ \"role\": \"user\", \"content\": \"Theo bạn, nên chọn chung cư tầng cao hay tầng thấp thì tốt hơn?\" },\n",
    "\t{ \"role\": \"assistant\", \"content\": \"[NORMAL_CHAT]\" },\n",
    "]\n",
    "\n",
    "def predict_intent(input):\n",
    "\tmessages = copy.deepcopy(template)\n",
    "\tmessages.append({ \"role\": \"user\", \"content\": input })\n",
    "\tresult = client.chat.completions.create(\n",
    "\t\tmodel=\"Qwen/Qwen2.5-72B-Instruct\", \n",
    "\t\tmessages=messages, \n",
    "\t\ttemperature=0.1,\n",
    "\t\tmax_tokens=1024,\n",
    "\t\ttop_p=0.7,\n",
    "\t\tstream=False\n",
    "\t)\n",
    "\treturn result.choices[0].message.content\n",
    "\n",
    "def metric(sample):\n",
    "\tif sample['label'] in sample['predicted_intent']:\n",
    "\t\treturn True\n",
    "\telse:\n",
    "\t\treturn False\n",
    "\n",
    "\n",
    "data['predicted_intent'] = data['text'].progress_apply(predict_intent)\n",
    "data['status'] = data.apply(metric, axis=1)\n",
    "print(\"accuracy:\",  (data['status'].sum() / len(data)) * 100)\n",
    "data.to_excel('data/router_generated.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
